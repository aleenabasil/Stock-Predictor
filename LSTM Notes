## LSTM
- analyze complex historical data using memory
- recurrent neural network that introduces parameters in RNN blocks every step
- contain variables called gates. gate values are determined based on information at that particular step, and prior gate information. this value is then multiplied by different variables of interest
- time series data: sequence of values collected at regular time intervals analyzed to track changes over time

## LSTM Architecture
- memory cell: contains gates, allow LSTM to remember and use past data
    - forget gate: decides what information to forget from the previous cell state
    - input gate: decides what new information to store in the current state cell
    - output gate: controls what information from the current cell state to output

- has ability to capture complex patterns and long term dependencies, making them valuable for stock prediction.

- lstm unit: 3 gates and one memory cell



# Plan

## Feature Engineering
- extract day, month, year and make new columns
- add open-close, low-high, and target columns
    - check if target is balanced or not using pie chart
    - check correlation between open, high, low, and close using heatmap
-------------------------------------------------------------
## Data Preprocessing and Normalization
- select features to train the model on
- transform values using fit transform
- scale data using min max scaler
- use 80% for training, 20% for testing

## Splitting the Data
- split data into training, validation, and test sets
- validation is necessary to assess how well the model can generalize on unseen data prior to testing on test set -- done to avoid overfitting and for hyperparameter tuning
    - overfitting is when the model learns patterns specific to the training set and cannot generalize to unseen data, and LSTM models are prone to overfitting
    - hyperparameter tuning lets us compare configurations (LSTM units, learning rates, activation functions) and we can select the best model based on validation performance rather than training

## Implement Model
- use keras models sequential
    - layers are added one after another, in a sequence
- use keras layers,  control # units, activation, input shape, mse, dense, adam optimizer, dropout
    - 2 lstm layers, implement drop out in between for regularization
    - number of units in parameter = 50, which is the number of LSTM neurons or memory cells 
    - activation: controls how signals flow, relu or tanh
    - input shape(1,1) = number of time steps, number of features
    - MSE = metric for mean squared error, measures how far predictions are from actual prices 
    - dense: output layer, units = 1 produces a single numerical output, taking the hidden state of LSTM and map to final price prediction
    - adam optimizer: adjust weights during training to min error
    - dropout: randomly drops a fraction of neurons during training (but not predicting) to prevent overfitting

- training
    - input = past stock price
    - output = next days stock price
    - shape: (batch size, time steps, features)
        - each batch has multiple samples
        - each sample looks at one day
        - each day has one feature

## Data Visualization
- use inverse transformation to get back to the original value with the transformed function 
- actual vs predicted open and close price graphs

## Challenges
- data quality and noise: unpredictable events (social media sentiment, news) difficult to differentiate and capture 
- limited data: our model has limited data available, and it is generally harder to predict them accurately for new companies. 
- overfitting: risk of overfitting means it performs well on past data but does not on future data.

